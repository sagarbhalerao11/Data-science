{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF2.0 Linear Classification",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_H9ETEXiRO6j"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load in the data\n",
        "from sklearn.datasets import load_breast_cancer"
      ],
      "metadata": {
        "id": "71DT_vDnRTF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the data\n",
        "data = load_breast_cancer()"
      ],
      "metadata": {
        "id": "ddVRyysqRVyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the type of 'data'\n",
        "type(data)"
      ],
      "metadata": {
        "id": "OVVbgQe9RV_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# note: it is a Bunch object\n",
        "# this basically acts like a dictionary where you can treat the keys like attributes\n",
        "data.keys()"
      ],
      "metadata": {
        "id": "se8E0iGwRe4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 'data' (the attribute) means the input data\n",
        "data.data.shape\n",
        "# it has 569 samples, 30 features"
      ],
      "metadata": {
        "id": "j8PuUPOERfIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 'targets'\n",
        "data.target\n",
        "# note how the targets are just 0s and 1s\n",
        "# normally, when you have K targets, they are labeled 0..K-1"
      ],
      "metadata": {
        "id": "xD9IG_bQRmOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# their meaning is not lost\n",
        "data.target_names"
      ],
      "metadata": {
        "id": "Hm43PnF5RqMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# there are also 569 corresponding targets\n",
        "data.target.shape"
      ],
      "metadata": {
        "id": "eM7OekS8Rvd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# you can also determine the meaning of each feature\n",
        "data.feature_names"
      ],
      "metadata": {
        "id": "MHYyrJcMRvrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# normally we would put all of our imports at the top\n",
        "# but this lets us tell a story\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# split the data into train and test sets\n",
        "# this lets us simulate how our model will perform in the future\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2)\n",
        "N, D = X_train.shape"
      ],
      "metadata": {
        "id": "rne_m66zR9sL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale the data\n",
        "# you'll learn why scaling is needed in a later course\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "NxiXQcPcSB9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now all the fun Tensorflow stuff\n",
        "# Build the model\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Input(shape=(D,)),\n",
        "  tf.keras.layers.Dense(10, activation='relu'),\n",
        "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n"
      ],
      "metadata": {
        "id": "h-nI1hgkSJPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Alternatively, you can do:\n",
        "# model = tf.keras.models.Sequential()\n",
        "# model.add(tf.keras.layers.Dense(1, input_shape=(D,), activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Train the model\n",
        "r = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100)\n",
        "\n",
        "\n",
        "# Evaluate the model - evaluate() returns loss and accuracy\n",
        "print(\"Train score:\", model.evaluate(X_train, y_train))\n",
        "print(\"Test score:\", model.evaluate(X_test, y_test))"
      ],
      "metadata": {
        "id": "HZ1R1mG5SObw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot what's returned by model.fit()\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(r.history['loss'], label='loss')\n",
        "plt.plot(r.history['val_loss'], label='val_loss')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "uYxBul-GSSQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the accuracy too\n",
        "plt.plot(r.history['accuracy'], label='acc')\n",
        "plt.plot(r.history['val_accuracy'], label='val_acc')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "zY798lCqSTUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 2: Making Predictions\n",
        "This goes with the lecture \"Making Predictions\""
      ],
      "metadata": {
        "id": "9imkuTF-Sedx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "P = model.predict(X_test)\n",
        "print(P) # they are outputs of the sigmoid, interpreted as probabilities p(y = 1 | x)"
      ],
      "metadata": {
        "id": "x4_2NowmSiQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Round to get the actual predictions\n",
        "# Note: has to be flattened since the targets are size (N,) while the predictions are size (N,1)\n",
        "import numpy as np\n",
        "P = np.round(P).flatten()\n",
        "print(P)"
      ],
      "metadata": {
        "id": "jdOQThquSl3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the accuracy, compare it to evaluate() output\n",
        "print(\"Manually calculated accuracy:\", np.mean(P == y_test))\n",
        "print(\"Evaluate output:\", model.evaluate(X_test, y_test))"
      ],
      "metadata": {
        "id": "83IUT-0qSp0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 3: Saving and Loading a Model\n",
        "This goes with the lecture \"Saving and Loading a Model\""
      ],
      "metadata": {
        "id": "79sTbQ6XSxlH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's now save our model to a file\n",
        "model.save('linearclassifier.h5')"
      ],
      "metadata": {
        "id": "sRQqbGfgStUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check that the model file exists\n",
        "!ls -lh "
      ],
      "metadata": {
        "id": "jivkmXgES1e2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's load the model and confirm that it still works\n",
        "# Note: there is a bug in Keras where load/save only works if you DON'T use the Input() layer explicitly\n",
        "# So, make sure you define the model with ONLY Dense(1, input_shape=(D,))\n",
        "# At least, until the bug is fixed\n",
        "# https://github.com/keras-team/keras/issues/10417\n",
        "model = tf.keras.models.load_model('linearclassifier.h5')\n",
        "print(model.layers)\n",
        "model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "id": "hVFxTdpXS46g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the file - requires Chrome (at this point)\n",
        "from google.colab import files\n",
        "files.download('linearclassifier.h5')"
      ],
      "metadata": {
        "id": "4G4agXd0S8jI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}